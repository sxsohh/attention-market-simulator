{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b787d1e1",
   "metadata": {},
   "source": [
    "# Attention Market Simulator â€” Notebook 01: Data Exploration\n",
    "\n",
    "This notebook explores the synthetic time series generated by `AttentionEnv`, \n",
    "which models human attention as a market microstructure system.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load the simulated data\n",
    "- Inspect attention, boredom, fatigue, and volatility over time\n",
    "- Compute attention \"returns\" and rolling volatility\n",
    "- Examine regime frequencies and summary statistics\n",
    "- Analyze attention imbalance vs future movement\n",
    "- Look for volatility clustering and basic autocorrelation\n",
    "\n",
    "This notebook is meant to look and feel like an exploratory quant research report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7cac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: nicer plots\n",
    "plt.style.use(\"default\")\n",
    "\n",
    "data_path = Path(\"..\") / \"data\" / \"attention_simulation.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7d203",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\\n\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nRegime counts:\")\n",
    "print(df[\"regime\"].value_counts())\n",
    "\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a45ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(df[\"t\"], df[\"attention_level\"])\n",
    "axes[0].set_ylabel(\"Attention\")\n",
    "\n",
    "axes[1].plot(df[\"t\"], df[\"volatility\"])\n",
    "axes[1].set_ylabel(\"Volatility\")\n",
    "\n",
    "axes[2].plot(df[\"t\"], df[\"attention_imbalance\"])\n",
    "axes[2].set_ylabel(\"Imbalance\")\n",
    "\n",
    "axes[3].plot(df[\"t\"], df[\"boredom\"], label=\"Boredom\")\n",
    "axes[3].plot(df[\"t\"], df[\"fatigue\"], label=\"Fatigue\")\n",
    "axes[3].set_ylabel(\"Boredom / Fatigue\")\n",
    "axes[3].set_xlabel(\"Time step\")\n",
    "axes[3].legend()\n",
    "\n",
    "fig.suptitle(\"Core State Variables Over Time\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88543218",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "regime_map = {\n",
    "    \"engaged\": 0,\n",
    "    \"fatigued\": 1,\n",
    "    \"overstimulated\": 2,\n",
    "    \"addictive_loop\": 3,\n",
    "    \"disengaged\": 4,\n",
    "    \"baseline\": 5,\n",
    "}\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    df[\"t\"],\n",
    "    df[\"attention_level\"],\n",
    "    c=df[\"regime\"].map(regime_map),\n",
    "    s=10,\n",
    ")\n",
    "\n",
    "ax.set_title(\"Attention Level with Regimes\")\n",
    "ax.set_xlabel(\"Time step\")\n",
    "ax.set_ylabel(\"Attention level\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30441a91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# One-step return in attention\n",
    "df[\"attn_return_1\"] = df[\"attention_level\"].diff()\n",
    "\n",
    "# Absolute return\n",
    "df[\"attn_abs_return_1\"] = df[\"attn_return_1\"].abs()\n",
    "\n",
    "# Forward 5-step change\n",
    "df[\"attn_fwd_change_5\"] = df[\"attention_level\"].shift(-5) - df[\"attention_level\"]\n",
    "\n",
    "# Direction label for classification later\n",
    "df[\"attn_fwd_up_5\"] = (df[\"attn_fwd_change_5\"] > 0).astype(int)\n",
    "\n",
    "# Rolling volatility (like realized vol)\n",
    "window = 20\n",
    "df[\"rolling_vol_20\"] = df[\"attn_return_1\"].rolling(window).std()\n",
    "\n",
    "# Rolling mean\n",
    "df[\"rolling_mean_20\"] = df[\"attention_level\"].rolling(window).mean()\n",
    "\n",
    "df[[\"attention_level\", \"attn_return_1\", \"attn_fwd_change_5\", \"rolling_vol_20\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e4dbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram of returns\n",
    "ax[0].hist(df[\"attn_return_1\"].dropna(), bins=50)\n",
    "ax[0].set_title(\"Distribution of 1-step Attention Returns\")\n",
    "ax[0].set_xlabel(\"Return\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Absolute returns (volatility proxy)\n",
    "ax[1].hist(df[\"attn_abs_return_1\"].dropna(), bins=50)\n",
    "ax[1].set_title(\"Distribution of |Return|\")\n",
    "ax[1].set_xlabel(\"|Return|\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9db7bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(df[\"t\"], df[\"attn_abs_return_1\"])\n",
    "ax.set_title(\"Absolute Attention Returns Over Time\")\n",
    "ax.set_xlabel(\"Time step\")\n",
    "ax.set_ylabel(\"|Return|\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecfc4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    \"attention_level\",\n",
    "    \"attn_return_1\",\n",
    "    \"attn_abs_return_1\",\n",
    "    \"rolling_vol_20\",\n",
    "    \"volatility\",\n",
    "    \"attention_imbalance\",\n",
    "]\n",
    "\n",
    "regime_summary = df.groupby(\"regime\")[group_cols].agg([\"mean\", \"std\", \"min\", \"max\", \"count\"])\n",
    "regime_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8a761",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.boxplot(data=df, x=\"regime\", y=\"attn_return_1\", ax=ax)\n",
    "ax.set_title(\"Distribution of 1-step Attention Returns by Regime\")\n",
    "ax.set_xlabel(\"Regime\")\n",
    "ax.set_ylabel(\"Return\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff257e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Drop NaNs from forward change\n",
    "analysis_df = df.dropna(subset=[\"attn_fwd_change_5\", \"attention_imbalance\"]).copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.scatterplot(\n",
    "    data=analysis_df.sample(min(1000, len(analysis_df)), random_state=42),\n",
    "    x=\"attention_imbalance\",\n",
    "    y=\"attn_fwd_change_5\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Attention Imbalance vs 5-step Forward Attention Change\")\n",
    "ax.set_xlabel(\"Current Imbalance\")\n",
    "ax.set_ylabel(\"Future 5-step Change\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "analysis_df[[\"attention_imbalance\", \"attn_fwd_change_5\"]].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70a259",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"attention_level\",\n",
    "    \"attn_return_1\",\n",
    "    \"attn_abs_return_1\",\n",
    "    \"rolling_vol_20\",\n",
    "    \"volatility\",\n",
    "    \"attention_imbalance\",\n",
    "    \"attention_liquidity\",\n",
    "    \"attention_demand\",\n",
    "    \"boredom\",\n",
    "    \"fatigue\",\n",
    "]\n",
    "\n",
    "corr = df[feature_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Matrix of Key Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ece19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plot_acf(df[\"attn_return_1\"].dropna(), lags=40, ax=ax)\n",
    "ax.set_title(\"ACF of 1-step Attention Returns\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
